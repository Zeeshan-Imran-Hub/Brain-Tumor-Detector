{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QsQyzDqQNiqNyZ3JxhT4Yws-ZphX1KOM",
      "authorship_tag": "ABX9TyMvcJxbgGUCBo3rtvRQbRfb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0RNy-H-4Lka"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import glob\n",
        "import cv2\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import models\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder, extensions=[\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.JPG\"]):\n",
        "    images = []\n",
        "    for ext in extensions:\n",
        "        for f in glob.iglob(os.path.join(folder, ext)):\n",
        "            img = cv2.imread(f)\n",
        "            if img is not None:\n",
        "                b, g, r = cv2.split(img)\n",
        "                img = cv2.merge([r, g, b])\n",
        "                images.append(img)\n",
        "    return images"
      ],
      "metadata": {
        "id": "vvEVj1rO43AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_random(healthy, tumor, num=5):\n",
        "    healthy_imgs = healthy[np.random.choice(healthy.shape[0], num, replace=False)]\n",
        "    tumor_imgs = tumor[np.random.choice(tumor.shape[0], num, replace=False)]\n",
        "\n",
        "    plt.figure(figsize=(16,9))\n",
        "    for i in range(num):\n",
        "        plt.subplot(1, num, i+1)\n",
        "        plt.title('healthy')\n",
        "        plt.imshow(healthy_imgs[i])\n",
        "\n",
        "    plt.figure(figsize=(16,9))\n",
        "    for i in range(num):\n",
        "        plt.subplot(1, num, i+1)\n",
        "        plt.title('tumor')\n",
        "        plt.imshow(tumor_imgs[i])\n"
      ],
      "metadata": {
        "id": "Ej5TusCl43xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MRIDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # Define the preprocessor pipeline, no need for ToPILImage since we already have numpy arrays\n",
        "        self.preprocessor = transforms.Compose([\n",
        "            transforms.ToTensor(),  # Convert image to tensor directly from numpy array\n",
        "            transforms.Resize((128, 128)),  # Resize to 128x128 (requires tensor, not PIL)\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n",
        "        ])\n",
        "\n",
        "        # Load images using the function\n",
        "        tumor_images = load_images_from_folder(\"/content/drive/MyDrive/Internship/data/yes/\")\n",
        "        no_tumor_images = load_images_from_folder(\"/content/drive/MyDrive/Internship/data/no/\")\n",
        "\n",
        "        # Labels\n",
        "        tumor_label = torch.ones(len(tumor_images), dtype=torch.float32)\n",
        "        healthy_label = torch.zeros(len(no_tumor_images), dtype=torch.float32)\n",
        "\n",
        "        # Apply preprocessing and convert to tensor\n",
        "        tumor = [self.preprocess_and_resize(img) for img in tumor_images]\n",
        "        healthy = [self.preprocess_and_resize(img) for img in no_tumor_images]\n",
        "\n",
        "        # Concatenate\n",
        "        self.images = torch.stack(tumor + healthy)\n",
        "        self.labels = torch.cat([tumor_label, healthy_label])\n",
        "\n",
        "    def preprocess_and_resize(self, img):\n",
        "        img_pil = Image.fromarray(img)\n",
        "        img_tensor = self.preprocessor(img_pil)\n",
        "        return img_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "YNmGTdak5FvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MRIDataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset.images, dataset.labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create train and test datasets\n",
        "train_dataset = MRIDataset()\n",
        "train_dataset.images = X_train\n",
        "train_dataset.labels = y_train\n",
        "\n",
        "test_dataset = MRIDataset()\n",
        "test_dataset.images = X_test\n",
        "test_dataset.labels = y_test\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "6gsIhEZt5Ky7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.cnn_model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=5),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=2, stride=5)\n",
        "        )\n",
        "\n",
        "        # Adjusting the in_features for the first linear layer\n",
        "        self.fc_model = nn.Sequential(\n",
        "            nn.Linear(in_features=16*4*4, out_features=120),  # 16*4*4 = 256\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.Linear(in_features=84, out_features=2)  # Binary classification, so 2 output features\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn_model(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layers\n",
        "        x = self.fc_model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "PJyB4yL65Ou2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, epochs):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_acc = calc_acc(train_loader, model)\n",
        "        test_acc = calc_acc(test_loader, model)\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}, Test Acc: {test_acc:.2f}')"
      ],
      "metadata": {
        "id": "xsjjU9ID5hkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_acc(loader,model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total"
      ],
      "metadata": {
        "id": "1omBrBR_5uPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    outputs, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            y_hat = model(images)\n",
        "            _, predicted = torch.max(y_hat, 1)\n",
        "            outputs.append(predicted.cpu().numpy())\n",
        "            true_labels.append(labels.cpu().numpy())\n",
        "    outputs = np.concatenate(outputs, axis=0)\n",
        "    true_labels = np.concatenate(true_labels, axis=0)\n",
        "    return outputs, true_labels"
      ],
      "metadata": {
        "id": "IvZ0hupi5nnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stats(outputs, y_true):\n",
        "    accuracy = accuracy_score(y_true, outputs)\n",
        "    precision = precision_score(y_true, outputs)\n",
        "    recall = recall_score(y_true, outputs)\n",
        "    cm = confusion_matrix(y_true, outputs)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)"
      ],
      "metadata": {
        "id": "TimrIp0FJ7ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vgg16():\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False  # Freeze all layers except the classifier\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, 2)  # Modify final layer for binary classification\n",
        "    return model\n",
        "\n",
        "def get_resnet50():\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False  # Freeze all layers except the final fully connected layer\n",
        "    model.fc = nn.Linear(model.fc.in_features, 2)  # Modify final layer for binary classification\n",
        "    return model"
      ],
      "metadata": {
        "id": "xL3azRWMNgit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = CNN()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn_model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "eta = 0.001\n",
        "EPOCH = 10\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=eta)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "Hd0MHkT7JWfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(cnn_model, optimizer, epochs=EPOCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLfk0AXvKktb",
        "outputId": "cac0bff0-03e1-41f6-c709-32f768c3d13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6497, Train Acc: 63.37, Test Acc: 52.94\n",
            "Epoch [2/10], Loss: 0.5812, Train Acc: 78.71, Test Acc: 74.51\n",
            "Epoch [3/10], Loss: 0.5449, Train Acc: 77.72, Test Acc: 82.35\n",
            "Epoch [4/10], Loss: 0.4539, Train Acc: 82.18, Test Acc: 82.35\n",
            "Epoch [5/10], Loss: 0.4338, Train Acc: 81.68, Test Acc: 80.39\n",
            "Epoch [6/10], Loss: 0.4243, Train Acc: 79.70, Test Acc: 80.39\n",
            "Epoch [7/10], Loss: 0.3944, Train Acc: 86.14, Test Acc: 84.31\n",
            "Epoch [8/10], Loss: 0.3214, Train Acc: 89.60, Test Acc: 88.24\n",
            "Epoch [9/10], Loss: 0.2374, Train Acc: 91.09, Test Acc: 84.31\n",
            "Epoch [10/10], Loss: 0.2074, Train Acc: 94.06, Test Acc: 88.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_outputs, CNN_true = evaluate_model(cnn_model, test_loader)"
      ],
      "metadata": {
        "id": "GPblWFz1KXod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Custom Neural Newtwork Metrics:\")\n",
        "stats(CNN_outputs, CNN_true)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEeOq-YhM39D",
        "outputId": "73f0af03-8d70-454c-84de-f060cd5f312c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Neural Newtwork Metrics:\n",
            "Accuracy: 0.8823529411764706\n",
            "Precision: 0.92\n",
            "Recall: 0.8518518518518519\n",
            "Confusion Matrix:\n",
            "[[22  2]\n",
            " [ 4 23]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_model = get_vgg16().to(device)\n",
        "optimizer_vgg = optim.Adam(vgg_model.classifier[6].parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "IdLP1MG8NRGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(vgg_model, optimizer_vgg, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8w_SZfTN0ZF",
        "outputId": "24109d46-0973-402a-9049-39525de25ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.7354, Train Acc: 73.76, Test Acc: 84.31\n",
            "Epoch [2/10], Loss: 0.4835, Train Acc: 80.69, Test Acc: 80.39\n",
            "Epoch [3/10], Loss: 0.5205, Train Acc: 83.17, Test Acc: 86.27\n",
            "Epoch [4/10], Loss: 0.3960, Train Acc: 86.14, Test Acc: 92.16\n",
            "Epoch [5/10], Loss: 0.4375, Train Acc: 91.58, Test Acc: 90.20\n",
            "Epoch [6/10], Loss: 0.3688, Train Acc: 89.60, Test Acc: 94.12\n",
            "Epoch [7/10], Loss: 0.3440, Train Acc: 93.07, Test Acc: 94.12\n",
            "Epoch [8/10], Loss: 0.3294, Train Acc: 92.57, Test Acc: 92.16\n",
            "Epoch [9/10], Loss: 0.3094, Train Acc: 94.06, Test Acc: 90.20\n",
            "Epoch [10/10], Loss: 0.2891, Train Acc: 96.04, Test Acc: 94.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_outputs, vgg_true = evaluate_model(vgg_model, test_loader)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lUPdnp6tN1SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"VGG-Net Metrics:\")\n",
        "stats(vgg_outputs, vgg_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5OTKphBejWR",
        "outputId": "75b02846-2d37-4dd5-f1b1-5e27c2e31e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG-Net Metrics:\n",
            "Accuracy: 0.9411764705882353\n",
            "Precision: 0.9615384615384616\n",
            "Recall: 0.9259259259259259\n",
            "Confusion Matrix:\n",
            "[[23  1]\n",
            " [ 2 25]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = get_resnet50().to(device)\n",
        "optimizer_resnet = optim.Adam(resnet_model.fc.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "cKyAWWLwRFhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(resnet_model, optimizer_resnet, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDYHZSaVRRVw",
        "outputId": "8675d0b0-280a-4479-c541-a9aff7ce40d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.6075, Train Acc: 78.71, Test Acc: 82.35\n",
            "Epoch [2/10], Loss: 0.3893, Train Acc: 81.68, Test Acc: 80.39\n",
            "Epoch [3/10], Loss: 0.3252, Train Acc: 89.60, Test Acc: 96.08\n",
            "Epoch [4/10], Loss: 0.3120, Train Acc: 88.12, Test Acc: 90.20\n",
            "Epoch [5/10], Loss: 0.2781, Train Acc: 91.58, Test Acc: 94.12\n",
            "Epoch [6/10], Loss: 0.2535, Train Acc: 92.57, Test Acc: 96.08\n",
            "Epoch [7/10], Loss: 0.2032, Train Acc: 92.08, Test Acc: 94.12\n",
            "Epoch [8/10], Loss: 0.1984, Train Acc: 95.54, Test Acc: 96.08\n",
            "Epoch [9/10], Loss: 0.2201, Train Acc: 97.03, Test Acc: 96.08\n",
            "Epoch [10/10], Loss: 0.2220, Train Acc: 98.02, Test Acc: 98.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_outputs, resnet_true = evaluate_model(resnet_model, test_loader)\n"
      ],
      "metadata": {
        "id": "ZK1fkIi2RR0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ResNet Metrics:\")\n",
        "\n",
        "stats(resnet_outputs, resnet_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRNLwC7-dr41",
        "outputId": "65faaa87-297f-4f8f-b5a6-c0a8f2a98ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet Metrics:\n",
            "Accuracy: 0.9803921568627451\n",
            "Precision: 0.9642857142857143\n",
            "Recall: 1.0\n",
            "Confusion Matrix:\n",
            "[[23  1]\n",
            " [ 0 27]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Custom Neural Newtwork Metrics:\")\n",
        "stats(CNN_outputs, CNN_true)\n",
        "\n",
        "print(\"\\nVGG-Net Metrics:\")\n",
        "stats(vgg_outputs, vgg_true)\n",
        "\n",
        "print(\"\\nResNet Metrics:\")\n",
        "stats(resnet_outputs, resnet_true)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sF2KjxJe3xC",
        "outputId": "e7053ec5-2385-4161-8251-49c6ba0ddbf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Neural Newtwork Metrics:\n",
            "Accuracy: 0.8823529411764706\n",
            "Precision: 0.92\n",
            "Recall: 0.8518518518518519\n",
            "Confusion Matrix:\n",
            "[[22  2]\n",
            " [ 4 23]]\n",
            "\n",
            "VGG-Net Metrics:\n",
            "Accuracy: 0.9411764705882353\n",
            "Precision: 0.9615384615384616\n",
            "Recall: 0.9259259259259259\n",
            "Confusion Matrix:\n",
            "[[23  1]\n",
            " [ 2 25]]\n",
            "\n",
            "ResNet Metrics:\n",
            "Accuracy: 0.9803921568627451\n",
            "Precision: 0.9642857142857143\n",
            "Recall: 1.0\n",
            "Confusion Matrix:\n",
            "[[23  1]\n",
            " [ 0 27]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = resnet_model\n",
        "torch.save(best_model, '/content/drive/MyDrive/Internship/best_brain_tumor_model.pth')"
      ],
      "metadata": {
        "id": "3bxqqBawfSBp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}